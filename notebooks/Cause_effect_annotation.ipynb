{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 266
    },
    "colab_type": "code",
    "id": "-mHy1y2kNsTR",
    "outputId": "0eea86fe-91ed-4b09-d8c0-b7c82acb6706"
   },
   "outputs": [],
   "source": [
    "!pip install stanza spacy scispacy nltk transformers --quiet\n",
    "!pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.2.4/en_ner_bc5cdr_md-0.2.4.tar.gz --quiet\n",
    "    \n",
    "#You may need to restart the shell after installation in order to run the subsequent codes and load the scispacy model correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ccxrncdOF_UU"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "import spacy\n",
    "import scispacy\n",
    "import nltk\n",
    "nltk.download('popular')\n",
    "nltk.download('brown')\n",
    "from textblob import TextBlob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from itertools import permutations \n",
    "nlp = spacy.load('en_ner_bc5cdr_md')\n",
    "from transformers import pipeline\n",
    "nlp_qa = pipeline('question-answering')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Dq4IxCSzrDn9"
   },
   "outputs": [],
   "source": [
    "#this function takes in a sentence, and returns a dictionary (if possible to extract)\n",
    "ce_count = 0\n",
    "sent_count = 0\n",
    "def get_ce_dict(sent):\n",
    "  global ce_count,sent_count \n",
    "  nps_tb = TextBlob(sent).noun_phrases   #can be substituted with an alternative way of extracting noun phrases\n",
    "  nps_spacy = [token.text for token in nlp(sent).noun_chunks]\n",
    "  nps = list(np.unique(nps_tb+nps_spacy))\n",
    "  curr_score = 0.8  #can be altered\n",
    "  curr_dict = np.nan\n",
    "  if(len(nps)>1):\n",
    "    perms = list(permutations(nps,2))\n",
    "    questions1 = [\"Is {} the cause of {}?\".format(a,b) for (a,b) in perms]\n",
    "    causes1 = [perm[0] for perm in perms]\n",
    "    effects1 = [perm[1] for perm in perms]\n",
    "    questions2 = [\"What is the cause of {}?\".format(a) for a in nps]\n",
    "    effects2 = nps\n",
    "    questions = questions1 + questions2\n",
    "    qa_input = [{'context':sent,'question':question} for question in questions]\n",
    "    results = nlp_qa(qa_input)\n",
    "    causes2 = [results[i]['answer'] for i in range(len(questions1),len(results))]\n",
    "    effects = effects1 +effects2 \n",
    "    causes = causes1 + causes2\n",
    "    for i in range(0,len(questions)):\n",
    "      result = dict(results[i])\n",
    "      cause = causes[i]\n",
    "      effect = effects[i]\n",
    "      if(result['score']>=curr_score):\n",
    "        #print(\"Pair found for sentence : {}\".format(sent))\n",
    "        #print(\" question:'{}',cause:'{}',effect:'{}',score:'{}'\".format(question,cause,effect,result['score']))\n",
    "        curr_dict  = result\n",
    "        curr_dict['cause'] = cause\n",
    "        curr_dict['effect'] = effect\n",
    "        curr_dict['question'] = questions[i]\n",
    "        curr_dict['noun_phrases'] = nps\n",
    "        curr_score = result['score']\n",
    "  if(curr_score>0.8):\n",
    "    sent_count += 1\n",
    "  ce_count+=1\n",
    "  if(ce_count%10==0):\n",
    "    print('sentences read == {}, sentences annotated = {}'.format(ce_count,sent_count))\n",
    "  return(curr_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "id": "GnlXFvRFG4_G",
    "outputId": "61c4e349-fbb7-4c81-e95a-bcd0a45ae6f4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pair found for sentence : the subjects were exposed to uv irradiation, causing a local tissue inflammation.\n",
      "\n",
      " cause:uv irradiation,,effect:a local tissue inflammation,score:0.9572199767599869\n",
      "Pair found for sentence : the subjects were exposed to uv irradiation, causing a local tissue inflammation.\n",
      "\n",
      " cause:uv irradiation,,effect:local tissue inflammation,score:0.9704209240399138\n",
      "Pair found for sentence : az12048189 did not, however, have any significant effects as assessed using mechanical stimulation or laser doppler.\n",
      "\n",
      " cause:mechanical stimulation,effect:laser doppler,score:0.975353403635566\n",
      "Pair found for sentence : this study also proved that trpv1 antagonists can inhibit a more complex, non-capsaicin dependent thermally induced pain signal.\n",
      "\n",
      " cause:trpv1 antagonists,effect:non-capsaicin dependent thermally induced pain signal,score:0.864427191783534\n",
      "Pair found for sentence : discomfort, severity and frequency of symptoms, visual symptoms, conjunctival injection, eyelid-meibomian gland findings, and corneal-tear signs were interpreted.\n",
      "\n",
      " cause:discomfort,,effect:severity,score:0.8823728895117711\n"
     ]
    }
   ],
   "source": [
    "n = 100  #set the number of sentences you want to analyze\n",
    "cr_df = pd.DataFrame()\n",
    "sents = open('227k_labeled_sentences.txt').readlines()\n",
    "cr_df[\"raw_sentence\"] = sents\n",
    "cr_df[\"tag\"] = cr_df[\"raw_sentence\"].apply(lambda x:x.split(' == ')[0])\n",
    "cr_df[\"raw_sentence\"] = cr_df[\"raw_sentence\"].apply(lambda x:x.split(' == ')[1])\n",
    "cr_df = cr_df.loc[cr_df[\"tag\"]=='1',:].iloc[0:n,:]\n",
    "sents_raw = cr_df[\"raw_sentence\"].values\n",
    "cr_df[\"HF_result\"] = cr_df[\"raw_sentence\"].apply(get_ce_dict)\n",
    "cr_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hAc_iRGvOwru"
   },
   "outputs": [],
   "source": [
    "cr_df = cr_df.dropna()\n",
    "cr_df[\"cause\"] = cr_df[\"HF_result\"].apply(lambda x:x[\"cause\"])\n",
    "cr_df[\"effect\"] = cr_df[\"HF_result\"].apply(lambda x:x[\"effect\"])\n",
    "cr_df[\"question\"] = cr_df[\"HF_result\"].apply(lambda x:x[\"question\"])\n",
    "cr_df[\"score\"] = cr_df[\"HF_result\"].apply(lambda x:round(x[\"score\"],3))\n",
    "cr_df[\"noun_phrases\"] = cr_df[\"HF_result\"].apply(lambda x:x[\"noun_phrases\"])\n",
    "cr_df[[\"raw_sentence\",\"cause\",\"effect\",\"question\",\"score\",\"noun_phrases\"]].to_csv(\"cr_df15.csv\",index=False)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "CR1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
